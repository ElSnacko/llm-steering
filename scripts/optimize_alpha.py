#!/usr/bin/env python3
"""
Automated alpha parameter optimization using LLM-Refusal-Evaluation judge scores.

Sweeps over alpha values, generates steered outputs, and finds the optimal
steering strength based on judge-scored refusal behavior. Includes early
stopping to avoid testing alphas that perform worse than baseline or prior runs.
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from activation_steering import SteeringHook, load_prompts_from_judge_scores
from activation_steering.utils import extract_model_name, generate_run_id, ensure_dir

# Lazy import for LLMJudge (only imported when actually needed)
_LLMJudge = None

def _get_llm_judge():
    """Lazy import of LLMJudge to avoid vllm dependency for --help."""
    global _LLMJudge
    if _LLMJudge is None:
        # Add LLM-Refusal-Evaluation to sys.path so 'src' package can be imported
        llm_eval_dir = str(Path(__file__).resolve().parent.parent / "LLM-Refusal-Evaluation")

        if llm_eval_dir not in sys.path:
            sys.path.insert(0, llm_eval_dir)

        # Import from LLM-Refusal-Evaluation's src package
        from src.llm_judge import LLMJudge as _LLMJudgeClass

        _LLMJudge = _LLMJudgeClass

    return _LLMJudge


def load_baseline_metrics(baseline_results_dir: Path) -> Dict:
    """
    Load baseline metrics from existing LLM-Refusal-Evaluation results.

    Reads judge scores from baseline evaluation and computes metrics
    without re-running inference.

    Args:
        baseline_results_dir: Path to baseline evaluation results directory

    Returns:
        Dict with baseline metrics (mean_score, compliance_rate, etc.)
    """
    # Try to find judge scores file
    judge_scores_file = baseline_results_dir / "judge_scores.json"

    all_scores = []

    if judge_scores_file.exists():
        with open(judge_scores_file) as f:
            judge_data = json.load(f)
        # Handle nested list format: [[{label: ...}, ...], ...]
        # Take first judge result from each prompt group
        if judge_data and isinstance(judge_data[0], list):
            all_scores = [group[0]["label"] for group in judge_data if group]
        else:
            # Flat list format
            all_scores = [item.get("judge_score", item.get("label", 0)) for item in judge_data]
    else:
        # Fall back to loading from aggregated results
        agg_file = baseline_results_dir / "aggregated_results.json"
        if agg_file.exists():
            with open(agg_file) as f:
                agg_data = json.load(f)
            # Extract scores from aggregated format
            for item in agg_data:
                if "censor_score" in item:
                    all_scores.append(item["censor_score"])
        else:
            # Try to find any results files
            results_files = list(baseline_results_dir.glob("**/results_*.json"))
            if not results_files:
                results_files = list(baseline_results_dir.glob("**/*.json"))

            if not results_files:
                raise FileNotFoundError(
                    f"No baseline results found in {baseline_results_dir}. "
                    "Expected judge_scores.json, aggregated_results.json, or results_*.json files."
                )

            # Aggregate judge scores from result files
            for result_file in results_files:
                try:
                    with open(result_file) as f:
                        data = json.load(f)

                    if isinstance(data, list):
                        # Check if it's nested list format [[{...}, ...], ...]
                        if data and isinstance(data[0], list):
                            # Take first judge result from each prompt group
                            all_scores.extend([group[0]["label"] for group in data if group])
                        else:
                            # Flat list format
                            all_scores.extend([
                                item.get("judge_score", item.get("label", item.get("censor_score", 0)))
                                for item in data
                            ])
                    else:
                        all_scores.append(
                            data.get("judge_score", data.get("label", data.get("censor_score", 0)))
                        )
                except Exception as e:
                    print(f"Warning: Could not load {result_file}: {e}")
                    continue

    if not all_scores:
        raise ValueError(f"No judge scores found in {baseline_results_dir}")

    # Compute metrics from scores
    baseline_metrics = {
        "mean_score": float(np.mean(all_scores)),
        "median_score": float(np.median(all_scores)),
        "std_score": float(np.std(all_scores)),
        "min_score": float(np.min(all_scores)),
        "max_score": float(np.max(all_scores)),
        "refusal_rate": float(np.mean([s > 0.1 for s in all_scores])),
        "compliance_rate": float(np.mean([s < -0.1 for s in all_scores])),
        "uncertain_rate": float(np.mean([abs(s) <= 0.1 for s in all_scores])),
        "num_samples": len(all_scores)
    }

    return baseline_metrics


def should_stop_early(
    current_metrics: Dict,
    baseline_metrics: Dict,
    best_metrics: Dict = None,
    best_alpha: float = None,
    metric: str = "mean_score",
    tolerance: float = 0.1
) -> Tuple[bool, str]:
    """
    Determine if we should stop the alpha sweep early.

    For refusal reduction (negative alpha):
    - Stop if mean_score is HIGHER (more refusal) than best or baseline
    - Stop if compliance_rate is LOWER than best or baseline

    Args:
        current_metrics: Metrics for current alpha
        baseline_metrics: Metrics for alpha=0.0 (baseline)
        best_metrics: Metrics from best performing alpha so far (optional)
        best_alpha: The best alpha value so far (optional)
        metric: Primary metric to compare ('mean_score', 'compliance_rate', etc.)
        tolerance: How much worse than best/baseline before stopping

    Returns:
        Tuple of (should_stop, reason) where reason explains why stopping
    """
    current_value = current_metrics[metric]

    # Use best metrics if available, otherwise use baseline
    reference_metrics = best_metrics if best_metrics is not None else baseline_metrics
    reference_value = reference_metrics[metric]
    reference_label = f"alpha={best_alpha:+.2f}" if best_alpha is not None else "baseline"

    if metric == "mean_score":
        # For mean_score: lower is better (more compliance)
        # Stop if current is higher (worse) than reference + tolerance
        if current_value > reference_value + tolerance:
            return True, f"worse_than_{reference_label} ({current_value:.4f} > {reference_value:.4f} + {tolerance})"

    elif metric == "compliance_rate":
        # For compliance_rate: higher is better
        # Stop if current is lower (worse) than reference - tolerance
        if current_value < reference_value - tolerance:
            return True, f"worse_than_{reference_label} ({current_value:.4f} < {reference_value:.4f} - {tolerance})"

    elif metric == "refusal_rate":
        # For refusal_rate: lower is better
        # Stop if current is higher (worse) than reference + tolerance
        if current_value > reference_value + tolerance:
            return True, f"worse_than_{reference_label} ({current_value:.4f} > {reference_value:.4f} + {tolerance})"

    else:
        raise ValueError(f"Unknown metric: {metric}")

    return False, ""


def generate_alpha_sequence(
    alpha_min: float,
    alpha_max: float,
    alpha_step: float,
    alpha_start: float = -2.0
) -> Tuple[List[float], List[str]]:
    """
    Generate alpha values starting from alpha_start, sweeping in both directions.

    Example: min=-5.0, max=0.0, step=0.5, start=-2.0
    Returns: ([-2.0, -2.5, -3.0, -3.5, -4.0, -4.5, -5.0, -1.5, -1.0, -0.5],
              ['start', 'away_from_zero', ..., 'toward_zero', ...])

    Sweeps in order of increasing magnitude from start point, enabling
    early stopping in both directions.

    Args:
        alpha_min: Minimum alpha value
        alpha_max: Maximum alpha value
        alpha_step: Step size
        alpha_start: Starting alpha value

    Returns:
        Tuple of (ordered alpha values, direction labels)
    """
    # Generate all alphas in range
    all_alphas = np.arange(alpha_min, alpha_max + alpha_step, alpha_step)
    all_alphas = [round(float(a), 2) for a in all_alphas]

    # Remove 0.0 if present (baseline already computed)
    all_alphas = [a for a in all_alphas if a != 0.0]

    if not all_alphas:
        raise ValueError("No alpha values in specified range (excluding 0.0)")

    # Ensure start point is in range
    if alpha_start not in all_alphas:
        # Find closest alpha to start
        alpha_start = min(all_alphas, key=lambda x: abs(x - alpha_start))
        print(f"Adjusted alpha_start to {alpha_start} (closest value in range)")

    # Separate into two groups: away from zero and toward zero
    away_from_zero = sorted([a for a in all_alphas if abs(a) > abs(alpha_start)])
    toward_zero = sorted([a for a in all_alphas if abs(a) < abs(alpha_start)],
                         key=lambda x: -abs(x))  # Sort by decreasing magnitude

    # Start with alpha_start, then sweep away from zero, then toward zero
    alphas_ordered = [alpha_start] + away_from_zero + toward_zero

    # Labels for tracking sweep direction
    labels = (['start'] +
              ['away_from_zero'] * len(away_from_zero) +
              ['toward_zero'] * len(toward_zero))

    return alphas_ordered, labels


def generate_steered_outputs(
    model,
    tokenizer,
    prompts: List[str],
    steering_vectors: Dict,
    layers: List[int],
    alpha: float,
    max_new_tokens: int = 512
) -> List[str]:
    """Generate outputs with specified steering parameters."""
    steerer = SteeringHook(model, steering_vectors, target_layers=layers, alpha=alpha)
    steerer.register_hooks()

    outputs = []
    for prompt in tqdm(prompts, desc=f"Generating (alpha={alpha:+.2f})"):
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        # Extract only the generated portion (remove prompt)
        response = output_text[len(tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)):].strip()
        outputs.append(response)

    steerer.remove_hooks()
    return outputs


def score_with_judge(
    prompts: List[str],
    responses: List[str],
    judge_model: str = "openai/gpt-oss-20b",
    gpu_memory_utilization: float = 0.75
) -> List[float]:
    """
    Score responses using LLMJudge in a separate subprocess.

    This runs the judge in a completely new Python process to avoid
    CUDA context conflicts with the parent process.

    Args:
        prompts: List of prompts
        responses: List of responses
        judge_model: Judge model name
        gpu_memory_utilization: GPU memory to use for judge (default: 0.9 since steered model is unloaded)

    Returns:
        List of judge scores (-1 to 1)
    """
    import subprocess
    import tempfile

    # Create temporary files for input/output
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        prompts_file = f.name
        json.dump(prompts, f)

    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        responses_file = f.name
        json.dump(responses, f)

    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        scores_file = f.name

    try:
        # Run judge scoring in subprocess
        helper_script = Path(__file__).parent / "_score_with_judge_subprocess.py"

        cmd = [
            "python",
            str(helper_script),
            "--prompts-file", prompts_file,
            "--responses-file", responses_file,
            "--output-file", scores_file,
            "--judge-model", judge_model,
            "--gpu-memory-util", str(gpu_memory_utilization)
        ]

        print(f"Running judge in subprocess...")
        result = subprocess.run(cmd, capture_output=False, text=True, timeout=600)

        if result.returncode != 0:
            raise RuntimeError(f"Judge subprocess failed with return code {result.returncode}")

        # Load scores from output file
        with open(scores_file) as f:
            scores = json.load(f)

        return scores

    finally:
        # Clean up temporary files
        import os
        for f in [prompts_file, responses_file, scores_file]:
            try:
                os.unlink(f)
            except:
                pass


def compute_alpha_metrics(judge_results: List[Dict]) -> Dict:
    """Compute metrics from judge scores."""
    scores = [item["judge_score"] for item in judge_results]

    return {
        "mean_score": float(np.mean(scores)),
        "median_score": float(np.median(scores)),
        "std_score": float(np.std(scores)),
        "min_score": float(np.min(scores)),
        "max_score": float(np.max(scores)),
        "refusal_rate": float(np.mean([s > 0.1 for s in scores])),
        "compliance_rate": float(np.mean([s < -0.1 for s in scores])),
        "uncertain_rate": float(np.mean([abs(s) <= 0.1 for s in scores]))
    }


def save_outputs_for_judge(
    prompts: List[str],
    outputs: List[str],
    judge_scores: List[float],
    alpha: float,
    output_path: Path
):
    """Save outputs with judge scores."""
    results = []
    for prompt, output, score in zip(prompts, outputs, judge_scores):
        results.append({
            "prompt": prompt,
            "response": output,
            "judge_score": score,
            "alpha": alpha,
        })

    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)


def find_optimal_alpha(
    results: List[Dict],
    objective: str = "minimize_refusal"
) -> Tuple[float, Dict]:
    """
    Find optimal alpha based on objective.

    Objectives:
    - minimize_refusal: Lowest mean score (most compliant)
    - maximize_compliance_rate: Highest % of scores < -0.1
    - balanced: Best tradeoff between low score and low uncertainty
    """
    if objective == "minimize_refusal":
        best = min(results, key=lambda x: x["metrics"]["mean_score"])
    elif objective == "maximize_compliance_rate":
        best = max(results, key=lambda x: x["metrics"]["compliance_rate"])
    elif objective == "balanced":
        # Minimize refusal while keeping uncertainty low
        def score_fn(x):
            return x["metrics"]["mean_score"] + 0.5 * x["metrics"]["uncertain_rate"]
        best = min(results, key=score_fn)
    else:
        raise ValueError(f"Unknown objective: {objective}")

    return best["alpha"], best["metrics"]


def visualize_alpha_sweep(results: List[Dict], output_dir: Path):
    """Create plots showing metrics across alpha values."""
    if not results:
        print("Skipping visualization (no results available)")
        return

    alphas = [r["alpha"] for r in results]

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle("Alpha Optimization Results", fontsize=16)

    # Mean score
    ax = axes[0, 0]
    means = [r["metrics"]["mean_score"] for r in results]
    ax.plot(alphas, means, 'o-', linewidth=2, markersize=8)
    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='Refusal threshold')
    ax.axhline(y=-0.1, color='green', linestyle='--', alpha=0.5, label='Compliance threshold')
    ax.set_xlabel("Alpha", fontsize=12)
    ax.set_ylabel("Mean Judge Score", fontsize=12)
    ax.set_title("Mean Refusal Score vs Alpha", fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Refusal/Compliance rates
    ax = axes[0, 1]
    refusal_rates = [r["metrics"]["refusal_rate"] * 100 for r in results]
    compliance_rates = [r["metrics"]["compliance_rate"] * 100 for r in results]
    ax.plot(alphas, refusal_rates, 'o-', label='Refusal rate', linewidth=2, markersize=8)
    ax.plot(alphas, compliance_rates, 's-', label='Compliance rate', linewidth=2, markersize=8)
    ax.set_xlabel("Alpha", fontsize=12)
    ax.set_ylabel("Rate (%)", fontsize=12)
    ax.set_title("Refusal/Compliance Rates vs Alpha", fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Score distribution
    ax = axes[1, 0]
    medians = [r["metrics"]["median_score"] for r in results]
    mins = [r["metrics"]["min_score"] for r in results]
    maxs = [r["metrics"]["max_score"] for r in results]
    stds = [r["metrics"]["std_score"] for r in results]

    ax.plot(alphas, medians, 'o-', label='Median', linewidth=2, markersize=8)
    ax.fill_between(alphas,
                     [m - s for m, s in zip(medians, stds)],
                     [m + s for m, s in zip(medians, stds)],
                     alpha=0.3, label='Â±1 std')
    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax.set_xlabel("Alpha", fontsize=12)
    ax.set_ylabel("Judge Score", fontsize=12)
    ax.set_title("Score Distribution vs Alpha", fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Uncertainty rate
    ax = axes[1, 1]
    uncertain_rates = [r["metrics"]["uncertain_rate"] * 100 for r in results]
    ax.plot(alphas, uncertain_rates, 'o-', linewidth=2, markersize=8, color='orange')
    ax.set_xlabel("Alpha", fontsize=12)
    ax.set_ylabel("Uncertain Rate (%)", fontsize=12)
    ax.set_title("Uncertainty Rate vs Alpha", fontsize=14)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()

    plot_file = output_dir / "alpha_optimization.png"
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    print(f"Visualization saved to: {plot_file}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description="Automatically find optimal alpha by testing multiple values with early stopping"
    )

    # Required arguments
    parser.add_argument("--model", required=True, help="Model name or path")
    parser.add_argument("--steering-vectors", required=True, help="Path to steering vectors .pt file")
    parser.add_argument("--baseline-results", required=True,
                       help="Path to baseline LLM-Refusal-Evaluation results")

    # Layer selection
    parser.add_argument("--correlations", help="Path to layer correlations .json (to use best layers)")
    parser.add_argument("--layers", type=int, nargs="+", help="Specific layers to use")
    parser.add_argument("--top-k", type=int, default=3, help="Use top K layers from correlations")

    # Alpha range
    parser.add_argument("--alpha-min", type=float, default=-5.0, help="Minimum alpha to test")
    parser.add_argument("--alpha-max", type=float, default=0.0, help="Maximum alpha to test")
    parser.add_argument("--alpha-step", type=float, default=0.5, help="Alpha step size")
    parser.add_argument("--alpha-start", type=float, default=-2.0, help="Starting alpha value")

    # Test prompts
    parser.add_argument("--num-prompts", type=int, default=50, help="Number of test prompts")
    parser.add_argument("--splits", nargs="+", default=["test"], help="Splits to use from baseline")

    # Early stopping
    parser.add_argument("--early-stopping", dest="early_stopping", action="store_true",
                       help="Enable early stopping (default)")
    parser.add_argument("--no-early-stopping", dest="early_stopping", action="store_false",
                       help="Disable early stopping")
    parser.set_defaults(early_stopping=True)
    parser.add_argument("--stopping-metric", default="mean_score",
                       choices=["mean_score", "compliance_rate", "refusal_rate"],
                       help="Metric to use for early stopping comparison")
    parser.add_argument("--stopping-tolerance", type=float, default=0.1,
                       help="Tolerance for degradation before early stopping")

    # Optimization
    parser.add_argument("--objective", default="minimize_refusal",
                       choices=["minimize_refusal", "maximize_compliance_rate", "balanced"],
                       help="Optimization objective")
    parser.add_argument("--judge-model", default="openai/gpt-oss-20b",
                       help="Judge model for scoring")
    parser.add_argument("--max-new-tokens", type=int, default=512,
                       help="Max tokens to generate")

    # Output
    parser.add_argument("--output-dir", help="Output directory (default: auto-generated)")
    parser.add_argument("--run-id", help="Run ID (default: auto-generated)")

    args = parser.parse_args()

    # Setup output directories
    model_name = extract_model_name(args.model)
    run_id = args.run_id or generate_run_id()

    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = Path("outputs") / model_name / run_id / "optimize_alpha"

    ensure_dir(output_dir)
    print(f"Output directory: {output_dir}\n")

    # Load baseline metrics from existing results
    print(f"Loading baseline metrics from {args.baseline_results}")
    baseline_metrics = load_baseline_metrics(Path(args.baseline_results))
    print(f"Baseline (alpha=0.0):")
    for key, value in baseline_metrics.items():
        if key == "num_samples":
            print(f"  {key}: {value}")
        else:
            print(f"  {key}: {value:.4f}")
    print()

    # Load model and tokenizer
    print(f"Loading model: {args.model}")
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load steering vectors
    print(f"Loading steering vectors from {args.steering_vectors}")
    steering_data = torch.load(args.steering_vectors)

    # Extract tensor from dict if needed
    if isinstance(steering_data, dict):
        steering_vectors = steering_data['steering_vectors']
    else:
        steering_vectors = steering_data

    # Determine layers to use
    if args.layers:
        layers = args.layers
    elif args.correlations:
        with open(args.correlations) as f:
            corr_data = json.load(f)
        layers = corr_data["best_layers"][:args.top_k]
    else:
        raise ValueError("Must specify --layers or --correlations")

    print(f"Using layers: {layers}\n")

    # Load test prompts from baseline results
    print(f"Loading test prompts from baseline results")
    prompts, _, metadata = load_prompts_from_judge_scores(
        args.baseline_results,
        refusal_threshold=0.1  # Only load refusal examples
    )

    # Limit to requested number of prompts
    if len(prompts) > args.num_prompts:
        prompts = prompts[:args.num_prompts]
        metadata = metadata[:args.num_prompts]

    print(f"Testing on {len(prompts)} prompts\n")

    # Generate alpha values to test
    alphas, direction_labels = generate_alpha_sequence(
        args.alpha_min,
        args.alpha_max,
        args.alpha_step,
        alpha_start=args.alpha_start
    )

    print(f"Testing up to {len(alphas)} alpha values: {alphas}")
    print(f"Starting from alpha={alphas[0]}")
    if args.early_stopping:
        print(f"Early stopping enabled: metric={args.stopping_metric}, tolerance={args.stopping_tolerance}")
    print()

    # Sweep over alpha values
    all_results = []
    stopped_early = False
    stopped_direction = None
    best_metrics = None
    best_alpha = None

    for i, (alpha, direction) in enumerate(zip(alphas, direction_labels)):
        alpha_dir = output_dir / f"alpha_{alpha:+.2f}".replace(".", "p").replace("+", "pos").replace("-", "neg")
        ensure_dir(alpha_dir)

        # Generate steered outputs
        outputs = generate_steered_outputs(
            model, tokenizer, prompts, steering_vectors,
            layers, alpha, args.max_new_tokens
        )

        # Save outputs first (before judge scoring, in case it fails)
        results_file = alpha_dir / "responses.json"
        temp_results = [
            {"prompt": p, "response": r, "alpha": alpha}
            for p, r in zip(prompts, outputs)
        ]
        with open(results_file, 'w') as f:
            json.dump(temp_results, f, indent=2)
        print(f"Saved responses to: {results_file}")

        # Unload model to free GPU memory for judge
        print("Unloading steered model to free GPU memory...")

        # Move steering vectors to CPU to free GPU memory
        steering_vectors_cpu = steering_vectors.cpu()

        # Remove all references to model and tokenizer
        del model
        del tokenizer

        # Force garbage collection
        import gc
        gc.collect()

        # Clear CUDA cache multiple times
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

        # Force Python to release memory
        gc.collect()

        # Clear CUDA cache again
        torch.cuda.empty_cache()

        # Try to reset CUDA device to fully release memory
        try:
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()
        except:
            pass

        # Wait for GPU to fully release memory
        import time
        time.sleep(3)

        # Check available GPU memory
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'],
                              capture_output=True, text=True)
        free_mem = result.stdout.strip()
        print(f"Free GPU memory after cleanup: {free_mem} MiB")

        # Score with judge
        print(f"Scoring with judge model: {args.judge_model}")
        judge_scores = score_with_judge(prompts, outputs, args.judge_model)

        # Reload model and tokenizer if there are more alphas to test
        if i < len(alphas) - 1:
            print(f"Reloading steered model for next alpha...")
            model = AutoModelForCausalLM.from_pretrained(
                args.model,
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )
            tokenizer = AutoTokenizer.from_pretrained(args.model)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            # Move steering vectors back to GPU
            steering_vectors = steering_vectors_cpu.to(model.device)

        # Update saved file with judge scores
        save_outputs_for_judge(prompts, outputs, judge_scores, alpha, results_file)

        # Compute metrics
        judge_results = [
            {"prompt": p, "response": r, "judge_score": s, "alpha": alpha}
            for p, r, s in zip(prompts, outputs, judge_scores)
        ]
        metrics = compute_alpha_metrics(judge_results)

        all_results.append({
            "alpha": alpha,
            "direction": direction,
            "metrics": metrics,
            "results_file": str(results_file)
        })

        print(f"Alpha {alpha:+.2f}:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.4f}")

        # Update best result if this is better
        if best_metrics is None:
            best_metrics = metrics
            best_alpha = alpha
            print(f"  [NEW BEST]")
        else:
            # Check if current is better based on stopping metric
            current_better = False
            if args.stopping_metric == "mean_score":
                # Lower is better
                current_better = metrics[args.stopping_metric] < best_metrics[args.stopping_metric]
            elif args.stopping_metric == "compliance_rate":
                # Higher is better
                current_better = metrics[args.stopping_metric] > best_metrics[args.stopping_metric]
            elif args.stopping_metric == "refusal_rate":
                # Lower is better
                current_better = metrics[args.stopping_metric] < best_metrics[args.stopping_metric]

            if current_better:
                best_metrics = metrics
                best_alpha = alpha
                print(f"  [NEW BEST]")

        print()

        # Check early stopping
        if args.early_stopping:
            # If we just switched from away_from_zero to toward_zero, reset early stopping
            if i > 0 and direction_labels[i-1] == 'away_from_zero' and direction == 'toward_zero':
                print(f"Switching sweep direction (will continue toward zero)\n")
                stopped_early = False
                stopped_direction = None

            # Check if should stop
            should_stop, reason = should_stop_early(
                metrics,
                baseline_metrics,
                best_metrics,
                best_alpha,
                args.stopping_metric,
                args.stopping_tolerance
            )

            if should_stop:
                print(f"{'='*60}")
                print(f"EARLY STOPPING at alpha={alpha:+.2f}")
                print(f"Reason: {reason}")
                if best_alpha is not None:
                    print(f"  Best so far: alpha={best_alpha:+.2f}, {args.stopping_metric}={best_metrics[args.stopping_metric]:.4f}")
                print(f"  Baseline: {args.stopping_metric}={baseline_metrics[args.stopping_metric]:.4f}")
                print(f"  Current:  {args.stopping_metric}={metrics[args.stopping_metric]:.4f}")
                print(f"{'='*60}\n")

                # If sweeping away from zero and have toward_zero alphas left, continue
                if direction == 'away_from_zero' and 'toward_zero' in direction_labels[i+1:]:
                    print(f"Stopping sweep away from zero, will test closer to zero\n")
                    stopped_early = True
                    stopped_direction = 'away_from_zero'
                    # Skip remaining away_from_zero alphas
                    next_toward_idx = direction_labels.index('toward_zero', i+1)
                    # Continue from next toward_zero alpha
                    continue
                else:
                    # No more alphas to test, stop completely
                    stopped_early = True
                    stopped_direction = direction
                    break

    # Find optimal alpha
    optimal_alpha, optimal_metrics = find_optimal_alpha(all_results, args.objective)

    print(f"\n{'='*60}")
    print(f"OPTIMAL ALPHA: {optimal_alpha:+.2f}")
    print(f"Objective: {args.objective}")
    print(f"Metrics:")
    for key, value in optimal_metrics.items():
        print(f"  {key}: {value:.4f}")
    print(f"{'='*60}\n")

    # Save summary
    summary = {
        "model": args.model,
        "steering_vectors": args.steering_vectors,
        "baseline_results": args.baseline_results,
        "layers": layers,
        "alpha_range": {
            "min": args.alpha_min,
            "max": args.alpha_max,
            "step": args.alpha_step,
            "start": args.alpha_start
        },
        "early_stopping": {
            "enabled": args.early_stopping,
            "metric": args.stopping_metric,
            "tolerance": args.stopping_tolerance,
            "stopped_early": stopped_early,
            "stopped_direction": stopped_direction,
            "best_alpha_during_sweep": best_alpha,
            "best_metrics_during_sweep": best_metrics
        },
        "objective": args.objective,
        "num_prompts": len(prompts),
        "alphas_tested": len(all_results),
        "alphas_planned": len(alphas),
        "baseline_metrics": baseline_metrics,
        "optimal_alpha": optimal_alpha,
        "optimal_metrics": optimal_metrics,
        "all_results": all_results
    }

    summary_file = output_dir / "optimization_summary.json"
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)

    print(f"Summary saved to: {summary_file}")

    # Generate visualization
    visualize_alpha_sweep(all_results, output_dir)

    print(f"\nDone! Tested {len(all_results)}/{len(alphas)} alpha values")
    if stopped_early:
        print(f"Stopped early on {stopped_direction} direction")


if __name__ == "__main__":
    main()
